import dotenv from "dotenv";
dotenv.config();

import cors from "cors";
import express from "express";
import multer from "multer";
import path from "path";
import { fileURLToPath } from "url";
import { spawn } from "child_process";
import fs from "fs";
import fsp from "fs/promises";
import * as sdk from "microsoft-cognitiveservices-speech-sdk";
import DeepSeek from "openai";
import * as wav from "node-wav";
import { pipeline, env } from "@xenova/transformers";
env.logLevel = "error";

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ çŽ¯å¢ƒå˜é‡æ£€æŸ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
["AZURE_SPEECH_KEY", "AZURE_SPEECH_REGION", "DEEPSEEK_API_KEY"].forEach(k => {
  if (!process.env[k]) {
    console.error(`âŒ ç¼ºå°‘çŽ¯å¢ƒå˜é‡ ${k}`);
    process.exit(1);
  }
});
const __dirname = path.dirname(fileURLToPath(import.meta.url));

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DeepSeek / Azure SDK åˆå§‹åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
const deepseek = new DeepSeek({
  apiKey: process.env.DEEPSEEK_API_KEY,
  baseURL: "https://api.deepseek.com/v1",
});
const baseSpeechConfig = sdk.SpeechConfig.fromSubscription(
  process.env.AZURE_SPEECH_KEY,
  process.env.AZURE_SPEECH_REGION
);
baseSpeechConfig.setProperty(
  sdk.PropertyId.SpeechServiceConnection_InitialSilenceTimeoutMs,
  "30000"
);
baseSpeechConfig.setProperty(
  sdk.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs,
  "30000"
);

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Express è®¾ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
const app = express();
const upload = multer({ dest: "uploads/" });
app.use(cors());
app.use(express.json());
app.use(express.static("."));

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
const shell = (cmd, args) =>
  new Promise((res, rej) => {
    const p = spawn(cmd, args, { stdio: "inherit" });
    p.on("close", c => (c === 0 ? res() : rej(new Error(`${cmd} exited ${c}`))));
  });

const fileExists = async p => !!(await fsp.stat(p).catch(() => 0));

async function saveOutput(id, data) {
  await fsp.mkdir("outputs", { recursive: true });
  const out = `outputs/${id}.json`;
  await fsp.writeFile(out, JSON.stringify(data, null, 2), "utf8");
  return out;
}

const toWav16k = (src, dst) =>
  shell("ffmpeg", [
    "-y",
    "-i",
    src,
    "-vn",
    "-ac",
    "1",
    "-ar",
    "16000",
    "-sample_fmt",
    "s16",
    "-c:a",
    "pcm_s16le",
    dst,
  ]);

const downloadYTAudio = (url, mp3) =>
  shell("yt-dlp", [
    "-f",
    "bestaudio",
    "-x",
    "--audio-format",
    "mp3",
    "--no-playlist",
    "--max-filesize",
    "200M",
    "-o",
    mp3,
    url,
  ]);

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¯­éŸ³è¯†åˆ«æ¨¡å— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
function azureTranscribeFromWav(wavPath, langTag = "zh") {
  return new Promise((resolve, reject) => {
    const localeMap = { zh: "zh-CN", "zh-Hant": "zh-TW", en: "en-US" };
    const speechConfig = baseSpeechConfig;
    speechConfig.speechRecognitionLanguage = localeMap[langTag] || langTag;
    speechConfig.setProperty(
      sdk.PropertyId.Speech_LogFilename,
      `azure_log_${Date.now()}.txt`
    );

    const audioConfig = sdk.AudioConfig.fromAudioFileInput(wavPath);
    const rec = new sdk.SpeechRecognizer(speechConfig, audioConfig);

    let fullTranscript = "";

    rec.recognized = (s, e) => {
      if (e.result.reason === sdk.ResultReason.RecognizedSpeech) {
        fullTranscript += e.result.text + " ";
      }
    };
    rec.canceled = (s, e) => {
      rec.stopContinuousRecognitionAsync();
      reject(new Error(e.errorDetails));
    };
    rec.sessionStopped = () => {
      rec.stopContinuousRecognitionAsync();
      resolve(fullTranscript.trim());
    };

    rec.startContinuousRecognitionAsync();
  });
}

async function whisperTranscribe(wavPath, langTag = "zh") {
  const stt = await pipeline(
    "automatic-speech-recognition",
    "Xenova/whisper-medium"
  );
  const buf = fs.readFileSync(wavPath);
  const { sampleRate, channelData } = wav.decode(buf);
  const res = await stt(channelData[0], {
    sampling_rate: sampleRate,
    language: langTag,
    task: "transcribe",
    chunk_length_s: 30,
    stride_length_s: 5,
  });
  return res.text.trim();
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å¤šè¯­è¨€æ‘˜è¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
const prompts = {
    zh: `
  è¯·å…ˆç”¨ **200-500 å­—**å®Œæ•´æ¦‚æ‹¬è¿™æ®µå†…å®¹ï¼Œç„¶åŽä¾åºç»†èŠ‚çš„åˆ†æžä»¥ä¸‹å››ä¸ªé—®é¢˜ï¼Œ
  æ¯ä¸ªé—®é¢˜ **100-150 å­—**ï¼Œå¹¶ä¿æŒç›¸åŒç¼–å·ç‹¬ç«‹æˆæ®µã€‚
  
  1. **æ•…äº‹çš„æ ¸å¿ƒé©±åŠ¨åŠ›æ˜¯ä»€ä¹ˆï¼Ÿ**ï¼ˆäººä¸Žå¤–ç•Œï¼ä»–äººï¼è‡ªæˆ‘çš„çŸ›ç›¾ï¼‰
  2. **äººç‰©å¡‘é€ ï¼š** ä¸»è§’æ˜¯è°ï¼Ÿæ ¸å¿ƒç›®æ ‡ï¼åŠ¨æœºï¼Ÿç»åŽ†äº†æ€Žæ ·çš„å˜åŒ–ï¼æˆé•¿ï¼Ÿï¼ˆé™æ€æˆ–åŠ¨æ€ï¼‰
  3. **ä¸»é¢˜æ€æƒ³ï¼š** æ•…äº‹çœŸæ­£æƒ³æŽ¢è®¨çš„æ·±å±‚å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆå…³äºŽäººæ€§ã€ç¤¾ä¼šã€ç”Ÿå‘½ç­‰ï¼‰
  4. **ä¸‰è€…å…³ç³»ï¼š** æƒ…èŠ‚å†²çªå¦‚ä½•æŽ¨åŠ¨äººç‰©è¡ŒåŠ¨ä¸Žå˜åŒ–ï¼Ÿå†²çªè§£å†³ä¸Žäººç‰©å˜åŒ–å¦‚ä½•å…±åŒæ­ç¤ºä¸»é¢˜ï¼Ÿ
  
  æ ¼å¼ï¼š
  **æ¦‚æ‹¬ï¼š** â€¦â€¦
  
  **1. æ ¸å¿ƒé©±åŠ¨åŠ›**  
  â€¦â€¦
  
  **2. äººç‰©å¡‘é€ **  
  â€¦â€¦
  
  **3. ä¸»é¢˜æ€æƒ³**  
  â€¦â€¦
  
  **4. ä¸‰è€…å…³ç³»**  
  â€¦â€¦
  `.trim(),
  
    "zh-Hant": `
  è«‹å…ˆç”¨ **200-500 å­—**å®Œæ•´æ¦‚æ‹¬é€™æ®µå…§å®¹ï¼Œç„¶å¾Œä¾åºç»†èŠ‚çš„åˆ†æžä»¥ä¸‹å››å€‹å•é¡Œï¼Œ
  æ¯å€‹å•é¡Œ **100-150 å­—**ï¼Œä¸¦ä¿æŒç›¸åŒç·¨è™Ÿç¨ç«‹æˆæ®µã€‚
  
  1. **æ•…äº‹çš„æ ¸å¿ƒé©…å‹•åŠ›æ˜¯ä»€éº¼ï¼Ÿ**ï¼ˆäººèˆ‡å¤–ç•Œï¼ä»–äººï¼è‡ªæˆ‘çš„çŸ›ç›¾ï¼‰
  2. **äººç‰©å¡‘é€ ï¼š** ä¸»è§’æ˜¯èª°ï¼Ÿæ ¸å¿ƒç›®æ¨™ï¼å‹•æ©Ÿï¼Ÿç¶“æ­·äº†æ€Žæ¨£çš„è®ŠåŒ–ï¼æˆé•·ï¼Ÿï¼ˆéœæ…‹æˆ–å‹•æ…‹ï¼‰
  3. **ä¸»é¡Œæ€æƒ³ï¼š** æ•…äº‹çœŸæ­£æƒ³æŽ¢è¨Žçš„æ·±å±¤å«ç¾©æ˜¯ä»€éº¼ï¼Ÿï¼ˆé—œæ–¼äººæ€§ã€ç¤¾æœƒã€ç”Ÿå‘½ç­‰ï¼‰
  4. **ä¸‰è€…é—œä¿‚ï¼š** æƒ…ç¯€è¡çªå¦‚ä½•æŽ¨å‹•äººç‰©è¡Œå‹•èˆ‡è®ŠåŒ–ï¼Ÿè¡çªè§£æ±ºèˆ‡äººç‰©è®ŠåŒ–å¦‚ä½•å…±åŒæ­ç¤ºä¸»é¡Œï¼Ÿ
  
  æ ¼å¼ï¼š
  **æ¦‚æ‹¬ï¼š** â€¦â€¦
  
  **1. æ ¸å¿ƒé©…å‹•åŠ›**  
  â€¦â€¦
  
  **2. äººç‰©å¡‘é€ **  
  â€¦â€¦
  
  **3. ä¸»é¡Œæ€æƒ³**  
  â€¦â€¦
  
  **4. ä¸‰è€…é—œä¿‚**  
  â€¦â€¦
  `.trim(),
  
    en: `
  First provide a **200â€“500-word** overall summary, then analyse the four points
  below in detail, **100â€“150 words** each, keeping the same numbering.
  
  1. **Core Driving Force:** Which conflict powers the story (man vs. environment / others / self)?
  2. **Characterisation:** Who is the protagonist? What is their main goal or motivation? How do they change (static vs. dynamic)?
  3. **Theme:** What deeper meaning does the story explore (humanity, society, life, etc.)?
  4. **Inter-relation:** How does the conflict drive actions and character change? How do the resolution and changes together reveal the theme?
  
  Format exactly as:
  
  **Summary:** â€¦â€¦
  
  **1. Core Driving Force**  
  â€¦â€¦
  
  **2. Characterisation**  
  â€¦â€¦
  
  **3. Theme**  
  â€¦â€¦
  
  **4. Inter-relation**  
  â€¦â€¦
  `.trim()
  };


async function summariseOne(text, lang = "zh") {
  const { choices } = await deepseek.chat.completions.create({
    model: "deepseek-chat",
    messages: [
      { role: "system", content: prompts[lang] || prompts.zh },
      { role: "user", content: text },
    ],
    max_tokens: 1800,
    temperature: 0.3,
  });
  return choices[0].message.content.trim();
}

const summariseMulti = async text => ({
  zh: await summariseOne(text, "zh"),
  "zh-Hant": await summariseOne(text, "zh-Hant"),
  en: await summariseOne(text, "en"),
});

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¼šè®®çºªè¦ Prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
const minutesPrompt = {
  zh: `
ä½ æ˜¯ä¸€åä¸“ä¸šä¼šè®®è®°å½•å‘˜ã€‚è¯·æŠŠä¸‹é¢çš„ä¼šè®®å…¨æ–‡è½¬å†™å†…å®¹æ•´ç†æˆã€Œä¼šè®®çºªè¦ã€ï¼Œ
ç”¨ **è¦ç‚¹å¼åˆ—è¡¨**ï¼Œå¹¶åŒ…å«å››ä¸ªåŒºå—ï¼š

â—Ž å…³é”®å†³ç­–  
â—Ž å¾…åŠžäº‹é¡¹ï¼ˆå†™æ¸…è´Ÿè´£äºº + æˆªæ­¢æ—¥æœŸï¼Œå¦‚æœªçŸ¥å¯å†™"å¾…å®š"ï¼‰  
â—Ž é‡è¦æ—¥æœŸ / é‡‘é¢ / é‡Œç¨‹ç¢‘  
â—Ž æœªè§£å†³é—®é¢˜æˆ–åŽç»­è®¨è®º

æ¯æ¡å‰é¢è¯·ç”¨ "â€¢" å¼€å¤´ã€‚ä»…è¾“å‡ºä¼šè®®çºªè¦ï¼Œä¸è¦æ·»åŠ å¤šä½™è¯´æ˜Žã€‚`.trim(),

  en: `
You are a professional meeting minutes writer. Turn the following full
transcript into concise meeting minutes with four sections:

â—Ž Key Decisions  
â—Ž Action Items (include owner + due date if known)  
â—Ž Important Dates / Numbers / Milestones  
â—Ž Open Questions & Next Steps

Bullet each line with "â€¢" and do NOT add commentary outside the four sections.`.trim(),

"zh-Hant": `
ä½ æ˜¯ä¸€åå°ˆæ¥­æœƒè­°è¨˜éŒ„å“¡ã€‚è«‹æŠŠä¸‹é¢çš„æœƒè­°å…¨æ–‡è½‰å¯«å…§å®¹æ•´ç†æˆã€Œæœƒè­°ç´€è¦ã€ï¼Œ  
ä½¿ç”¨ **è¦é»žå¼åˆ—è¡¨**ï¼Œä¸¦åˆ†æˆå››å€‹å€å¡Šï¼š

â—Ž é—œéµæ±ºç­–  
â—Ž å¾…è¾¦äº‹é …ï¼ˆè¨»æ˜Žè² è²¬äºº + æˆªæ­¢æ—¥æœŸï¼Œå¦‚æœªçŸ¥å¯å¯«ã€Œå¾…å®šã€ï¼‰  
â—Ž é‡è¦æ—¥æœŸ / é‡‘é¡ / é‡Œç¨‹ç¢‘  
â—Ž æœªè§£æ±ºå•é¡Œæˆ–å¾ŒçºŒè¨Žè«–

æ¯æ¢å‰é¢è«‹ç”¨ "â€¢" é–‹é ­ã€‚åƒ…è¼¸å‡ºæœƒè­°ç´€è¦ï¼Œä¸è¦æ·»åŠ å¤šé¤˜èªªæ˜Žã€‚`.trim()
};

async function generateMinutes(text, lang="zh") {
  const { choices } = await deepseek.chat.completions.create({
    model      : "deepseek-chat",
    messages   : [
      { role:"system", content: minutesPrompt[lang] || minutesPrompt.zh },
      { role:"user",   content: text }
    ],
    max_tokens : 1200,
    temperature: 0.3
  });
  return choices[0].message.content.trim();
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å¤šè¯­è¨€ä¼šè®®çºªè¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
const generateMinutesMulti = async (text) => ({
  zh: await generateMinutes(text, "zh"),
  "zh-Hant": await generateMinutes(text, "zh-Hant"),
  en: await generateMinutes(text, "en"),
});

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¸»æŽ¥å£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
app.post("/api/process", upload.single("media"), async (req, res) => {
  try {
    const {
      mode = "transcript",
      language = "zh",
      videoUrl,
      reuseId,
      transcript: bodyTranscript,
      engine: userEngine, 
    } = req.body;

    let transcript = bodyTranscript;
    let sttEngine = "";
    let fileData = null;

    if (reuseId) {
        const p = `outputs/${reuseId}.json`;
        if (await fileExists(p)) {
            fileData = JSON.parse(await fsp.readFile(p, 'utf8'));
            
            if (!transcript) transcript = fileData.transcript || '';
            sttEngine = fileData.engine || "reuse";

            if (mode === 'summary' && fileData.summary) {
            return res.json({ id: reuseId, transcript, summary: fileData.summary, engine: fileData.engine });
            }

            if (mode === 'minutes' && fileData.minutes) {
            return res.json({ id: reuseId, transcript, minutes: fileData.minutes, engine: fileData.engine });
            }
        }
    }

    // no transcript then download
    let wavPath = req.file?.path;
    if (!transcript && !wavPath && videoUrl) {
      const ts = Date.now(),
        mp3 = `uploads/${ts}.mp3`;
      await downloadYTAudio(videoUrl, mp3);
      wavPath = `uploads/${ts}.wav`;
      await toWav16k(mp3, wavPath);
      await fsp.unlink(mp3);
    }
    // audio recognize
    if (!transcript && wavPath) {
        const prefer = userEngine || "whisper";
        if (prefer === "azure") {
          try {
            transcript = await azureTranscribeFromWav(wavPath, language);
            sttEngine  = "azure";
          } catch {
            transcript = await whisperTranscribe(wavPath, language);
            sttEngine  = "whisper";
          }
        } else {
          try {
            transcript = await whisperTranscribe(wavPath, language);
            sttEngine  = "whisper";
          } catch {
            transcript = await azureTranscribeFromWav(wavPath, language);
            sttEngine  = "azure";
          }
        }
        await fsp.unlink(wavPath);
      }

    if (!transcript)
      return res
        .status(400)
        .json({ error: "æœªä¸Šä¼ éŸ³é¢‘ï¼Œä¹Ÿæœªæä¾› videoUrl / transcript / reuseIdã€‚" });

    // save full texts
    if (mode === "transcript") {
      const id = Date.now();
      await saveOutput(id, { transcript, engine: sttEngine });
      return res.json({ id, transcript, engine: sttEngine});
    }
    // generate summary
    if (mode === "summary") {
        const summary = await summariseMulti(transcript);
        const id = reuseId || Date.now();
        await saveOutput(id, { ...(fileData||{}), transcript, summary, engine: sttEngine });
        return res.json({ id, transcript, summary, engine: sttEngine });
      }
    
    // minutes
    if (mode === "minutes") {
      const minutes = await generateMinutesMulti(transcript);
      const id = reuseId || Date.now();
      await saveOutput(id, { transcript, minutes, engine: sttEngine });
      return res.json({id, transcript, minutes, engine: sttEngine});
    }
    return res.status(400).json({ error: `æœªçŸ¥æ¨¡å¼ ${mode}` });

  } catch (err) {
    console.error("âŒ å‡ºé”™ï¼š", err);
    res.status(500).json({ error: err.message });
  }
});

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å¯åŠ¨æœåŠ¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
const PORT = process.env.PORT || 3000;
app.listen(PORT, () => console.log(`ðŸš€ Service listening on :${PORT}`));
